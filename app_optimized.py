# app.py ‚Äî –¥–ª—è Streamlit Cloud (–º–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è, –±–µ–∑ —Å–∏–Ω–∏—Ö —Ñ–æ—Ç–æ)
import streamlit as st
import cv2
import easyocr
import pandas as pd
import re
import io
import numpy as np
from ultralytics import YOLO
from collections import Counter

st.set_page_config(page_title="–ü—Ä–æ–ø—É—Å–∫–∏ ‚Äî –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è", layout="centered")  # ‚Üê layout="centered" –ª—É—á—à–µ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö
st.title("üß† –£–º–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤")

# === –ö–ï–®–ò ===
@st.cache_resource
def load_model():
    model_path = 'best.pt'  # ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å!
    return YOLO(model_path)

@st.cache_resource
def load_ocr():
    return easyocr.Reader(['ru'], gpu=False)

model = load_model()
reader = load_ocr()


# === –£–ú–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ===
class NameFilter:
    STOP_WORDS = {
        "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π", "—Å—Ç—É–¥–µ–Ω—Ç", "—É—á–∞—Å—Ç–Ω–∏–∫", "—Å–æ—Ç—Ä—É–¥–Ω–∏–∫", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
        "–Ω–∞", "–ø–æ", "–≤", "–∏–∑", "–æ—Ç", "–¥–æ", "–∑–∞", "—Å", "–∫", "—É",
        "–∏", "–∏–ª–∏", "–Ω–æ", "–∞", "–∂–µ",
        "–Ω–æ–º–µ—Ä", "–ø—Ä–æ–ø—É—Å–∫", "–∫–∞—Ä—Ç–∞", "—Ñ–æ—Ç–æ", "–¥–∞—Ç–∞", "–≤—ã–¥–∞—á–∞",
        "–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω", "–ø–æ–¥–ø–∏—Å—å", "–ø–µ—á–∞—Ç—å", "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è",
        "–∞–æ", "–æ–æ", "–∑–∞–æ", "–ø–∞–æ", "–Ω–∫–æ", "–∏–ø",
    }
    
    NAME_PATTERNS = [
        r'^[–ê-–Ø–Å][–∞-—è—ë]{1,20}$',
        r'^[–ê-–Ø–Å][–∞-—è—ë]+[- ][–ê-–Ø–Å][–∞-—è—ë]+$', 
    ]
    
    COMMON_FIRST_NAMES = {
        "–∞–Ω–¥—Ä–µ–π", "–∞–ª–µ–∫—Å–µ–π", "–∞–ª–µ–∫—Å–∞–Ω–¥—Ä", "–∞—Ä—Ç–µ–º", "–∞—Ä—Ç—ë–º", "–±–æ—Ä–∏—Å", "–≤–∞–¥–∏–º",
        "–≤–∞–ª–µ–Ω—Ç–∏–Ω", "–≤–∞–ª–µ—Ä–∏–π", "–≤–∞—Å–∏–ª–∏–π", "–≤–∏–∫—Ç–æ—Ä", "–≤–ª–∞–¥–∏–º–∏—Ä", "–≤–ª–∞–¥–∏—Å–ª–∞–≤",
        "–≥–µ–Ω–Ω–∞–¥–∏–π", "–≥–µ–æ—Ä–≥–∏–π", "–≥—Ä–∏–≥–æ—Ä–∏–π", "–¥–∞–Ω–∏–∏–ª", "–¥–µ–Ω–∏—Å", "–¥–º–∏—Ç—Ä–∏–π",
        "–µ–≤–≥–µ–Ω–∏–π", "–µ–≥–æ—Ä", "–∏–≤–∞–Ω", "–∏–≥–æ—Ä—å", "–∫–∏—Ä–∏–ª–ª", "–∫–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω",
        "–ª–µ–≤", "–ª–µ–æ–Ω–∏–¥", "–º–∞–∫—Å–∏–º", "–º–∏—Ö–∞–∏–ª", "–Ω–∏–∫–æ–ª–∞–π", "–æ–ª–µ–≥",
        "–ø–∞–≤–µ–ª", "–ø–µ—Ç—Ä", "–ø—ë—Ç—Ä", "—Ä–æ–º–∞–Ω", "—Ä—É—Å–ª–∞–Ω", "—Å–µ—Ä–≥–µ–π",
        "—Å—Ç–∞–Ω–∏—Å–ª–∞–≤", "—Å—Ç–µ–ø–∞–Ω", "—Ç–∏–º–æ—Ñ–µ–π", "—Ñ–µ–¥–æ—Ä", "—Ñ—ë–¥–æ—Ä", "—é—Ä–∏–π",
        "—è—Ä–æ—Å–ª–∞–≤",
        "–∞–ª–µ–Ω–∞", "–∞–ª—ë–Ω–∞", "–∞–ª–∏–Ω–∞", "–∞–ª–ª–∞", "–∞–Ω–∞—Å—Ç–∞—Å–∏—è", "–∞–Ω–≥–µ–ª–∏–Ω–∞",
        "–∞–Ω–Ω–∞", "–∞–Ω—Ç–æ–Ω–∏–Ω–∞", "–≤–∞–ª–µ–Ω—Ç–∏–Ω–∞", "–≤–∞–ª–µ—Ä–∏—è", "–≤–µ—Ä–∞", "–≤–∏–∫—Ç–æ—Ä–∏—è",
        "–≥–∞–ª–∏–Ω–∞", "–¥–∞—Ä—å—è", "–¥–∏–∞–Ω–∞", "–µ–≤–≥–µ–Ω–∏—è", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–∞", "–µ–ª–µ–Ω–∞",
        "–µ–ª–∏–∑–∞–≤–µ—Ç–∞", "–∂–∏–Ω–∞—Ä", "–∑–∏–Ω–∞–∏–¥–∞", "–∏–Ω–Ω–∞", "–∏—Ä–∏–Ω–∞", "–∫—Ä–∏—Å—Ç–∏–Ω–∞",
        "–∫—Å–µ–Ω–∏—è", "–∫—Å—ë–Ω–∏—è", "–ª–∞—Ä–∏—Å–∞", "–ª—é–±–æ–≤—å", "–ª—é–¥–º–∏–ª–∞", "–º–∞—Ä–∏–Ω–∞",
        "–º–∞—Ä–∏—è", "–º–∞—Ä–≥–∞—Ä–∏—Ç–∞", "–Ω–∞–¥–µ–∂–¥–∞", "–Ω–∞—Ç–∞–ª—å—è", "–Ω–∞—Ç–∞–ª–∏—è", "–æ–∫—Å–∞–Ω–∞",
        "–æ–ª—å–≥–∞", "–ø–æ–ª–∏–Ω–∞", "—Å–≤–µ—Ç–ª–∞–Ω–∞", "—Å–æ—Ñ–∏—è", "—Å–æ—Ñ—å—è", "—Ç–∞–º–∞—Ä–∞",
        "—Ç–∞—Ç—å—è–Ω–∞", "—é–ª–∏—è", "—è–Ω–∞",
    }
    
    COMMON_LAST_NAMES = { –≤–∞—à —Å–ø–∏—Å–æ–∫ —Ñ–∞–º–∏–ª–∏–π –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π }

    @staticmethod
    def is_stop_word(word):
        word_lower = word.lower()
        if len(word) < 2: return True
        if re.match(r'^\d+$', word): return True
        if re.match(r'^[^–∞-—è–ê-–Ø—ë–Å]+$', word): return True
        if word_lower in NameFilter.STOP_WORDS: return True
        if word.isupper() and len(word) > 3: return True
        if re.match(r'.*—É–Ω–∏–≤–µ—Ä.*', word_lower): return True
        return False

    @staticmethod
    def is_likely_name_part(word):
        if NameFilter.is_stop_word(word): return False
        word_lower = word.lower()
        for pattern in NameFilter.NAME_PATTERNS:
            if re.match(pattern, word): return True
        if (word_lower in NameFilter.COMMON_FIRST_NAMES or 
            word_lower in NameFilter.COMMON_LAST_NAMES): return True
        if (word[0].isupper() and len(word) >= 3 and word.isalpha() and
            not any(ch.isdigit() for ch in word)):
            endings = ['–æ–≤', '–µ–≤', '–∏–Ω', '—ã–Ω', '–æ–≤–∞', '–µ–≤–∞', '–∏–Ω–∞', '—ã–Ω–∞',
                       '–∏–π', '–æ–π', '–∞—è', '—è—è', '–ª—å', '–¥—Ä–∞', '–ª–∞', '—Ç–∞']
            for ending in endings:
                if word_lower.endswith(ending):
                    return True
        return False

    @staticmethod
    def extract_fio_from_lines(lines):
        all_words = []
        for line in lines:
            words = re.split(r'[\s,.;:]+', line.strip())
            all_words.extend(words)
        candidates = [w for w in all_words if NameFilter.is_likely_name_part(w)]
        if len(candidates) >= 2:
            fio_parts = []
            for w in candidates:
                if w not in fio_parts:
                    fio_parts.append(w)
            return " ".join(fio_parts[:3])
        return None


# === OCR –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ ===
def preprocess_for_ocr(image):
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image

    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)
    denoised = cv2.bilateralFilter(enhanced, 9, 75, 75)
    _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    scale = 1.5
    new_size = (int(binary.shape[1] * scale), int(binary.shape[0] * scale))
    resized = cv2.resize(binary, new_size, interpolation=cv2.INTER_CUBIC)
    return resized


def extract_text_with_context(card_image):
    processed = preprocess_for_ocr(card_image)
    try:
        detailed_results = reader.readtext(
            processed, detail=1, paragraph=False, width_ths=0.7, ycenter_ths=0.5
        )
        all_texts = [text.strip() for _, text, conf in detailed_results if conf > 0.1]
        fio = NameFilter.extract_fio_from_lines(all_texts)
        if fio:
            return fio, all_texts
        likely_names = []
        for text in all_texts:
            words = text.split()
            name_words = [w for w in words if NameFilter.is_likely_name_part(w)]
            likely_names.extend(name_words)
        if len(likely_names) >= 2:
            return " ".join(likely_names[:3]), all_texts
        return None, all_texts
    except Exception as e:
        st.warning(f"–û—à–∏–±–∫–∞ OCR: {e}")
        return None, []


# === –û–ë–†–ê–ë–û–¢–ö–ê –û–î–ù–û–ì–û –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø (—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ü–≤–µ—Ç–∞) ===
def process_single_image_and_display(image, filename, show_debug=True):
    results = []
    
    if show_debug:
        st.subheader(f"üì∑ {filename}")
        col1, col2 = st.columns(2)
        with col1:
            # ‚úÖ BGR ‚Üí RGB (—Ñ–æ—Ç–æ –Ω–µ —Å–∏–Ω–∏–µ!)
            st.image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), caption="–ò—Å—Ö–æ–¥–Ω–æ–µ", use_container_width=True)
    
    yolo_results = model(image, conf=0.4, verbose=False)
    
    if show_debug and hasattr(yolo_results[0], 'plot'):
        with col2:
            plotted = yolo_results[0].plot()
            # ‚úÖ BGR ‚Üí RGB
            plotted_rgb = cv2.cvtColor(plotted, cv2.COLOR_BGR2RGB)
            st.image(plotted_rgb, caption="–î–µ—Ç–µ–∫—Ü–∏–∏", use_container_width=True)
    
    boxes = yolo_results[0].boxes
    cards_found = len(boxes) if boxes is not None else 0
    
    if show_debug:
        st.caption(f"üì¶ –ù–∞–π–¥–µ–Ω–æ: {cards_found} –ø—Ä–æ–ø—É—Å–∫–æ–≤")

    if boxes is not None:
        for i, box in enumerate(boxes):
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            card = image[y1:y2, x1:x2]
            
            if show_debug:
                with st.expander(f"–ü—Ä–æ–ø—É—Å–∫ {i+1}"):
                    st.image(cv2.cvtColor(card, cv2.COLOR_BGR2RGB), caption="–í—ã—Ä–µ–∑–∞–Ω–Ω—ã–π", use_container_width=True)
            
            fio, all_texts = extract_text_with_context(card)
            
            if fio:
                results.append(fio)
                if show_debug:
                    st.success(f"‚úÖ {fio}")
            elif show_debug:
                st.warning("–§–ò–û –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    elif show_debug:
        st.warning("‚ùå –ü—Ä–æ–ø—É—Å–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return results


# === –ö–≠–® –≠–ö–°–ü–û–†–¢–ê ===
@st.cache_data(ttl=300)
def prepare_export_files(edited_df):
    excel_buffer = io.BytesIO()
    edited_df.to_excel(excel_buffer, index=False, engine='openpyxl')
    excel_bytes = excel_buffer.getvalue()
    txt_content = "\n".join(
        edited_df["–§–ò–û"]
        .dropna()
        .astype(str)
        .str.strip()
        .where(lambda x: x != "")
        .dropna()
        .tolist()
    )
    return excel_bytes, txt_content


# === –ò–ù–¢–ï–†–§–ï–ô–° ===
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
debug_mode = st.sidebar.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –æ—Ç–ª–∞–¥–∫—É", False)  # ‚Üê –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False ‚Äî —á–∏—â–µ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ

uploaded_files = st.file_uploader(
    "üì∏ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–æ—Ç–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤",
    type=["jpg", "jpeg", "png"],
    accept_multiple_files=True,
    help="–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: —á—ë—Ç–∫–∏–µ —Ñ–æ—Ç–æ, –ø—Ä–æ–ø—É—Å–∫–∏ –∫—Ä—É–ø–Ω–æ"
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
if 'all_fios' not in st.session_state:
    st.session_state.all_fios = []
if 'processed' not in st.session_state:
    st.session_state.processed = False

# –û–±—Ä–∞–±–æ—Ç–∫–∞ ‚Äî –æ–¥–∏–Ω —Ä–∞–∑
if uploaded_files and not st.session_state.processed:
    st.session_state.all_fios = []
    
    for idx, uploaded_file in enumerate(uploaded_files):
        file_bytes = uploaded_file.getvalue()
        nparr = np.frombuffer(file_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        if img is None:
            st.error(f"‚ùó –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å: {uploaded_file.name}")
            continue
        
        fios = process_single_image_and_display(img, uploaded_file.name, debug_mode)
        st.session_state.all_fios.extend(fios)
    
    st.session_state.processed = True

# === –í–´–í–û–î ‚Äî –≤—Å–µ–≥–¥–∞, –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ ===
if st.session_state.processed:
    all_fios = st.session_state.all_fios
    
    if all_fios:
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ, –±–µ–∑ –¥—É–±–ª–µ–π
        unique_fios = []
        seen = set()
        for fio in all_fios:
            if fio not in seen:
                seen.add(fio)
                unique_fios.append(fio)
        
        st.markdown("---")
        st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        st.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(unique_fios)} –§–ò–û")
        
        df_editable = pd.DataFrame(unique_fios, columns=["–§–ò–û"])
        edited_df = st.data_editor(
            df_editable,
            num_rows="dynamic",
            use_container_width=True,
            key="fio_editor"
        )
        
        final_list = edited_df["–§–ò–û"].dropna().astype(str).str.strip()
        final_list = final_list[final_list != ""].tolist()
        
        # –≠–∫—Å–ø–æ—Ä—Ç
        excel_bytes, txt_content = prepare_export_files(edited_df)
        
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                "üì• Excel",
                excel_bytes,
                "—É—á–∞—Å—Ç–Ω–∏–∫–∏.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        with col2:
            st.download_button(
                "üì• TXT",
                txt_content,
                "—É—á–∞—Å—Ç–Ω–∏–∫–∏.txt",
                "text/plain"
            )
    
    else:
        st.markdown("---")
        st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        st.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –§–ò–û")

# –°–±—Ä–æ—Å –ø—Ä–∏ –Ω–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
if not uploaded_files:
    st.session_state.processed = False
